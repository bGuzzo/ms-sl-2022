y = Importance, fill = Importance)) +
geom_bar(stat='identity') +
geom_text(aes(x = Variables, y = 0.5, label = Rank),
hjust=0, vjust=0.55, size = 4, colour = 'red') +
labs(x = 'Regressors') +
coord_flip() +
theme_classic()
predictions <- predict(rf_model, newdata = test_set)
cm2<-confusionMatrix(predictions, as.factor(y_test))
cm2
plot(rf_model, ylim=c(0,0.50), xlim=c(0,500))
legend('topright', colnames(rf_model$err.rate), col=1:3, fill=1:3)
plot(rf_model, ylim=c(0,0.50), xlim=c(0,140))
legend('topright', colnames(rf_model$err.rate), col=1:3, fill=1:3)
rf_model = randomForest(as.factor(y_training)~. , data = training_set, ntree =96, importance = TRUE)
# Classifica di importanza dei vari regressori ai fini della predizione
importance <- importance(rf_model)
varImportance <- data.frame(Variables = row.names(importance),
Importance = round(importance[ ,'MeanDecreaseGini'],2))
rankImportance <- varImportance %>%
mutate(Rank = paste0('#',dense_rank(desc(Importance))))
# Visualizziamo il tutto con ggplot
ggplot(rankImportance, aes(x = reorder(Variables, Importance),
y = Importance, fill = Importance)) +
geom_bar(stat='identity') +
geom_text(aes(x = Variables, y = 0.5, label = Rank),
hjust=0, vjust=0.55, size = 4, colour = 'red') +
labs(x = 'Regressors') +
coord_flip() +
theme_classic()
predictions <- predict(rf_model, newdata = test_set)
cm2<-confusionMatrix(predictions, as.factor(y_test))
cm2
plot(rf_model, ylim=c(0,0.50), xlim=c(0,500))
legend('topright', colnames(rf_model$err.rate), col=1:3, fill=1:3)
plot(rf_model, ylim=c(0,0.50), xlim=c(0,140))
legend('topright', colnames(rf_model$err.rate), col=1:3, fill=1:3)
rf_model = randomForest(as.factor(y_training)~. , data = training_set, ntree =100, importance = TRUE)
# Classifica di importanza dei vari regressori ai fini della predizione
importance <- importance(rf_model)
varImportance <- data.frame(Variables = row.names(importance),
Importance = round(importance[ ,'MeanDecreaseGini'],2))
rankImportance <- varImportance %>%
mutate(Rank = paste0('#',dense_rank(desc(Importance))))
# Visualizziamo il tutto con ggplot
ggplot(rankImportance, aes(x = reorder(Variables, Importance),
y = Importance, fill = Importance)) +
geom_bar(stat='identity') +
geom_text(aes(x = Variables, y = 0.5, label = Rank),
hjust=0, vjust=0.55, size = 4, colour = 'red') +
labs(x = 'Regressors') +
coord_flip() +
theme_classic()
predictions <- predict(rf_model, newdata = test_set)
cm2<-confusionMatrix(predictions, as.factor(y_test))
cm2
plot(rf_model, ylim=c(0,0.50), xlim=c(0,500))
legend('topright', colnames(rf_model$err.rate), col=1:3, fill=1:3)
set.seed(100)
x <- DataSet
#Standardizzazione
#la standardizzazione non è necessaria per i regressori con valori caratterizzati da una scala limitata
#regressori da standardizzare
x_toscale <- x[, c("age", "trtbps", "chol", "thalachh", "oldpeak")]
#regressori da non standardizzare
dropped <- x[,  c("sex", "cp", "fbs", "restecg", "exng", "slp", "caa", "thall")]
head(dropped)
head(x_toscale)
scaled_regressors <- scale(x_toscale) #standardizziamo
head(scaled_regressors)
cleaned_regressors <- cbind(scaled_regressors, dropped, label = x$output) #dataset completo standardizzato
head(cleaned_regressors)
#divisione in training set e test set
index <- sample(1:nrow(cleaned_regressors), 0.7*nrow(cleaned_regressors))
training_set <- cleaned_regressors[index, ]
y_training <- training_set[, "label"] #var dipendente training
training_set <- training_set[,-14]
test_set <- cleaned_regressors[-index, ]
y_test <- test_set[, "label"] #var dipendente test
test_set <- test_set[, -14] #eliminiamo la variabile dipendente dal test set
head(test_set)
dim(training_set)
dim(test_set)
summary(training_set)
summary(test_set)
############# RANDOM FOREST CLASSIFIER ################
rf_model = randomForest(as.factor(y_training)~. , data = training_set, ntree =100, importance = TRUE)
# Classifica di importanza dei vari regressori ai fini della predizione
importance <- importance(rf_model)
varImportance <- data.frame(Variables = row.names(importance),
Importance = round(importance[ ,'MeanDecreaseGini'],2))
rankImportance <- varImportance %>%
mutate(Rank = paste0('#',dense_rank(desc(Importance))))
# Visualizziamo il tutto con ggplot
ggplot(rankImportance, aes(x = reorder(Variables, Importance),
y = Importance, fill = Importance)) +
geom_bar(stat='identity') +
geom_text(aes(x = Variables, y = 0.5, label = Rank),
hjust=0, vjust=0.55, size = 4, colour = 'red') +
labs(x = 'Regressors') +
coord_flip() +
theme_classic()
predictions <- predict(rf_model, newdata = test_set)
cm2<-confusionMatrix(predictions, as.factor(y_test))
cm2
set.seed(100)
x <- DataSet
#Standardizzazione
#la standardizzazione non è necessaria per i regressori con valori caratterizzati da una scala limitata
#regressori da standardizzare
x_toscale <- x[, c("age", "trtbps", "chol", "thalachh", "oldpeak")]
#regressori da non standardizzare
dropped <- x[,  c("sex", "cp", "fbs", "restecg", "exng", "slp", "caa", "thall")]
head(dropped)
head(x_toscale)
scaled_regressors <- scale(x_toscale) #standardizziamo
head(scaled_regressors)
cleaned_regressors <- cbind(scaled_regressors, dropped, label = x$output) #dataset completo standardizzato
head(cleaned_regressors)
#divisione in training set e test set
index <- sample(1:nrow(cleaned_regressors), 0.7*nrow(cleaned_regressors))
training_set <- cleaned_regressors[index, ]
y_training <- training_set[, "label"] #var dipendente training
training_set <- training_set[,-14]
test_set <- cleaned_regressors[-index, ]
y_test <- test_set[, "label"] #var dipendente test
test_set <- test_set[, -14] #eliminiamo la variabile dipendente dal test set
head(test_set)
dim(training_set)
dim(test_set)
summary(training_set)
summary(test_set)
############# RANDOM FOREST CLASSIFIER ################
rf_model = randomForest(as.factor(y_training)~. , data = training_set, ntree =100, importance = TRUE)
# Classifica di importanza dei vari regressori ai fini della predizione
importance <- importance(rf_model)
varImportance <- data.frame(Variables = row.names(importance),
Importance = round(importance[ ,'MeanDecreaseGini'],2))
rankImportance <- varImportance %>%
mutate(Rank = paste0('#',dense_rank(desc(Importance))))
# Visualizziamo il tutto con ggplot
ggplot(rankImportance, aes(x = reorder(Variables, Importance),
y = Importance, fill = Importance)) +
geom_bar(stat='identity') +
geom_text(aes(x = Variables, y = 0.5, label = Rank),
hjust=0, vjust=0.55, size = 4, colour = 'red') +
labs(x = 'Regressors') +
coord_flip() +
theme_classic()
predictions <- predict(rf_model, newdata = test_set)
cm2<-confusionMatrix(predictions, as.factor(y_test))
cm2
plot(rf_model, ylim=c(0,0.50), xlim=c(0,500))
legend('topright', colnames(rf_model$err.rate), col=1:3, fill=1:3)
plot(rf_model, ylim=c(0,0.50), xlim=c(0,140))
legend('topright', colnames(rf_model$err.rate), col=1:3, fill=1:3)
set.seed(100)
x <- DataSet
#Standardizzazione
#la standardizzazione non è necessaria per i regressori con valori caratterizzati da una scala limitata
#regressori da standardizzare
x_toscale <- x[, c("age", "trtbps", "chol", "thalachh", "oldpeak")]
#regressori da non standardizzare
dropped <- x[,  c("sex", "cp", "fbs", "restecg", "exng", "slp", "caa", "thall")]
head(dropped)
head(x_toscale)
scaled_regressors <- scale(x_toscale) #standardizziamo
head(scaled_regressors)
cleaned_regressors <- cbind(scaled_regressors, dropped, label = x$output) #dataset completo standardizzato
head(cleaned_regressors)
#divisione in training set e test set
index <- sample(1:nrow(cleaned_regressors), 0.7*nrow(cleaned_regressors))
training_set <- cleaned_regressors[index, ]
y_training <- training_set[, "label"] #var dipendente training
training_set <- training_set[,-14]
test_set <- cleaned_regressors[-index, ]
y_test <- test_set[, "label"] #var dipendente test
test_set <- test_set[, -14] #eliminiamo la variabile dipendente dal test set
head(test_set)
dim(training_set)
dim(test_set)
summary(training_set)
summary(test_set)
############# RANDOM FOREST CLASSIFIER ################
rf_model = randomForest(as.factor(y_training)~. , data = training_set, ntree =56, importance = TRUE)
# Classifica di importanza dei vari regressori ai fini della predizione
importance <- importance(rf_model)
varImportance <- data.frame(Variables = row.names(importance),
Importance = round(importance[ ,'MeanDecreaseGini'],2))
rankImportance <- varImportance %>%
mutate(Rank = paste0('#',dense_rank(desc(Importance))))
# Visualizziamo il tutto con ggplot
ggplot(rankImportance, aes(x = reorder(Variables, Importance),
y = Importance, fill = Importance)) +
geom_bar(stat='identity') +
geom_text(aes(x = Variables, y = 0.5, label = Rank),
hjust=0, vjust=0.55, size = 4, colour = 'red') +
labs(x = 'Regressors') +
coord_flip() +
theme_classic()
predictions <- predict(rf_model, newdata = test_set)
cm2<-confusionMatrix(predictions, as.factor(y_test))
cm2
plot(rf_model, ylim=c(0,0.50), xlim=c(0,500))
legend('topright', colnames(rf_model$err.rate), col=1:3, fill=1:3)
plot(rf_model, ylim=c(0,0.50), xlim=c(0,140))
legend('topright', colnames(rf_model$err.rate), col=1:3, fill=1:3)
set.seed(100)
x <- DataSet
#Standardizzazione
#la standardizzazione non è necessaria per i regressori con valori caratterizzati da una scala limitata
#regressori da standardizzare
x_toscale <- x[, c("age", "trtbps", "chol", "thalachh", "oldpeak")]
#regressori da non standardizzare
dropped <- x[,  c("sex", "cp", "fbs", "restecg", "exng", "slp", "caa", "thall")]
head(dropped)
head(x_toscale)
scaled_regressors <- scale(x_toscale) #standardizziamo
head(scaled_regressors)
cleaned_regressors <- cbind(scaled_regressors, dropped, label = x$output) #dataset completo standardizzato
head(cleaned_regressors)
#divisione in training set e test set
index <- sample(1:nrow(cleaned_regressors), 0.7*nrow(cleaned_regressors))
training_set <- cleaned_regressors[index, ]
y_training <- training_set[, "label"] #var dipendente training
training_set <- training_set[,-14]
test_set <- cleaned_regressors[-index, ]
y_test <- test_set[, "label"] #var dipendente test
test_set <- test_set[, -14] #eliminiamo la variabile dipendente dal test set
head(test_set)
dim(training_set)
dim(test_set)
summary(training_set)
summary(test_set)
############# RANDOM FOREST CLASSIFIER ################
rf_model = randomForest(as.factor(y_training)~. , data = training_set, ntree =57, importance = TRUE)
# Classifica di importanza dei vari regressori ai fini della predizione
importance <- importance(rf_model)
varImportance <- data.frame(Variables = row.names(importance),
Importance = round(importance[ ,'MeanDecreaseGini'],2))
rankImportance <- varImportance %>%
mutate(Rank = paste0('#',dense_rank(desc(Importance))))
# Visualizziamo il tutto con ggplot
ggplot(rankImportance, aes(x = reorder(Variables, Importance),
y = Importance, fill = Importance)) +
geom_bar(stat='identity') +
geom_text(aes(x = Variables, y = 0.5, label = Rank),
hjust=0, vjust=0.55, size = 4, colour = 'red') +
labs(x = 'Regressors') +
coord_flip() +
theme_classic()
predictions <- predict(rf_model, newdata = test_set)
cm2<-confusionMatrix(predictions, as.factor(y_test))
cm2
plot(rf_model, ylim=c(0,0.50), xlim=c(0,500))
legend('topright', colnames(rf_model$err.rate), col=1:3, fill=1:3)
plot(rf_model, ylim=c(0,0.50), xlim=c(0,140))
legend('topright', colnames(rf_model$err.rate), col=1:3, fill=1:3)
set.seed(100)
x <- DataSet
#Standardizzazione
#la standardizzazione non è necessaria per i regressori con valori caratterizzati da una scala limitata
#regressori da standardizzare
x_toscale <- x[, c("age", "trtbps", "chol", "thalachh", "oldpeak")]
#regressori da non standardizzare
dropped <- x[,  c("sex", "cp", "fbs", "restecg", "exng", "slp", "caa", "thall")]
head(dropped)
head(x_toscale)
scaled_regressors <- scale(x_toscale) #standardizziamo
head(scaled_regressors)
cleaned_regressors <- cbind(scaled_regressors, dropped, label = x$output) #dataset completo standardizzato
head(cleaned_regressors)
#divisione in training set e test set
index <- sample(1:nrow(cleaned_regressors), 0.7*nrow(cleaned_regressors))
training_set <- cleaned_regressors[index, ]
y_training <- training_set[, "label"] #var dipendente training
training_set <- training_set[,-14]
test_set <- cleaned_regressors[-index, ]
y_test <- test_set[, "label"] #var dipendente test
test_set <- test_set[, -14] #eliminiamo la variabile dipendente dal test set
head(test_set)
dim(training_set)
dim(test_set)
summary(training_set)
summary(test_set)
############# RANDOM FOREST CLASSIFIER ################
rf_model = randomForest(as.factor(y_training)~. , data = training_set, ntree =57, importance = TRUE)
# Classifica di importanza dei vari regressori ai fini della predizione
importance <- importance(rf_model)
varImportance <- data.frame(Variables = row.names(importance),
Importance = round(importance[ ,'MeanDecreaseGini'],2))
rankImportance <- varImportance %>%
mutate(Rank = paste0('#',dense_rank(desc(Importance))))
# Visualizziamo il tutto con ggplot
ggplot(rankImportance, aes(x = reorder(Variables, Importance),
y = Importance, fill = Importance)) +
geom_bar(stat='identity') +
geom_text(aes(x = Variables, y = 0.5, label = Rank),
hjust=0, vjust=0.55, size = 4, colour = 'red') +
labs(x = 'Regressors') +
coord_flip() +
theme_classic()
predictions <- predict(rf_model, newdata = test_set)
cm2<-confusionMatrix(predictions, as.factor(y_test))
cm2
plot(rf_model, ylim=c(0,0.50), xlim=c(0,500))
legend('topright', colnames(rf_model$err.rate), col=1:3, fill=1:3)
plot(rf_model, ylim=c(0,0.50), xlim=c(0,140))
legend('topright', colnames(rf_model$err.rate), col=1:3, fill=1:3)
set.seed(100)
x <- DataSet
#Standardizzazione
#la standardizzazione non è necessaria per i regressori con valori caratterizzati da una scala limitata
#regressori da standardizzare
x_toscale <- x[, c("age", "trtbps", "chol", "thalachh", "oldpeak")]
#regressori da non standardizzare
dropped <- x[,  c("sex", "cp", "fbs", "restecg", "exng", "slp", "caa", "thall")]
head(dropped)
head(x_toscale)
scaled_regressors <- scale(x_toscale) #standardizziamo
head(scaled_regressors)
cleaned_regressors <- cbind(scaled_regressors, dropped, label = x$output) #dataset completo standardizzato
head(cleaned_regressors)
#divisione in training set e test set
index <- sample(1:nrow(cleaned_regressors), 0.7*nrow(cleaned_regressors))
training_set <- cleaned_regressors[index, ]
y_training <- training_set[, "label"] #var dipendente training
training_set <- training_set[,-14]
test_set <- cleaned_regressors[-index, ]
y_test <- test_set[, "label"] #var dipendente test
test_set <- test_set[, -14] #eliminiamo la variabile dipendente dal test set
head(test_set)
dim(training_set)
dim(test_set)
summary(training_set)
summary(test_set)
############# RANDOM FOREST CLASSIFIER ################
rf_model = randomForest(as.factor(y_training)~. , data = training_set, ntree =58, importance = TRUE)
# Classifica di importanza dei vari regressori ai fini della predizione
importance <- importance(rf_model)
varImportance <- data.frame(Variables = row.names(importance),
Importance = round(importance[ ,'MeanDecreaseGini'],2))
rankImportance <- varImportance %>%
mutate(Rank = paste0('#',dense_rank(desc(Importance))))
# Visualizziamo il tutto con ggplot
ggplot(rankImportance, aes(x = reorder(Variables, Importance),
y = Importance, fill = Importance)) +
geom_bar(stat='identity') +
geom_text(aes(x = Variables, y = 0.5, label = Rank),
hjust=0, vjust=0.55, size = 4, colour = 'red') +
labs(x = 'Regressors') +
coord_flip() +
theme_classic()
predictions <- predict(rf_model, newdata = test_set)
cm2<-confusionMatrix(predictions, as.factor(y_test))
cm2
plot(rf_model, ylim=c(0,0.50), xlim=c(0,500))
legend('topright', colnames(rf_model$err.rate), col=1:3, fill=1:3)
plot(rf_model, ylim=c(0,0.50), xlim=c(0,140))
legend('topright', colnames(rf_model$err.rate), col=1:3, fill=1:3)
rf_model = randomForest(as.factor(y_training)~. , data = training_set, ntree =450, importance = TRUE)
# Classifica di importanza dei vari regressori ai fini della predizione
importance <- importance(rf_model)
varImportance <- data.frame(Variables = row.names(importance),
Importance = round(importance[ ,'MeanDecreaseGini'],2))
rankImportance <- varImportance %>%
mutate(Rank = paste0('#',dense_rank(desc(Importance))))
# Visualizziamo il tutto con ggplot
ggplot(rankImportance, aes(x = reorder(Variables, Importance),
y = Importance, fill = Importance)) +
geom_bar(stat='identity') +
geom_text(aes(x = Variables, y = 0.5, label = Rank),
hjust=0, vjust=0.55, size = 4, colour = 'red') +
labs(x = 'Regressors') +
coord_flip() +
theme_classic()
predictions <- predict(rf_model, newdata = test_set)
cm2<-confusionMatrix(predictions, as.factor(y_test))
cm2
plot(rf_model, ylim=c(0,0.50), xlim=c(0,500))
legend('topright', colnames(rf_model$err.rate), col=1:3, fill=1:3)
plot(rf_model, ylim=c(0,0.50), xlim=c(0,140))
legend('topright', colnames(rf_model$err.rate), col=1:3, fill=1:3)
set.seed(100)
x <- DataSet
#Standardizzazione
#la standardizzazione non è necessaria per i regressori con valori caratterizzati da una scala limitata
#regressori da standardizzare
x_toscale <- x[, c("age", "trtbps", "chol", "thalachh", "oldpeak")]
#regressori da non standardizzare
dropped <- x[,  c("sex", "cp", "fbs", "restecg", "exng", "slp", "caa", "thall")]
head(dropped)
head(x_toscale)
scaled_regressors <- scale(x_toscale) #standardizziamo
head(scaled_regressors)
cleaned_regressors <- cbind(scaled_regressors, dropped, label = x$output) #dataset completo standardizzato
head(cleaned_regressors)
#divisione in training set e test set
index <- sample(1:nrow(cleaned_regressors), 0.7*nrow(cleaned_regressors))
training_set <- cleaned_regressors[index, ]
y_training <- training_set[, "label"] #var dipendente training
training_set <- training_set[,-14]
test_set <- cleaned_regressors[-index, ]
y_test <- test_set[, "label"] #var dipendente test
test_set <- test_set[, -14] #eliminiamo la variabile dipendente dal test set
head(test_set)
dim(training_set)
dim(test_set)
summary(training_set)
summary(test_set)
############# RANDOM FOREST CLASSIFIER ################
rf_model = randomForest(as.factor(y_training)~. , data = training_set, ntree =450, importance = TRUE)
# Classifica di importanza dei vari regressori ai fini della predizione
importance <- importance(rf_model)
varImportance <- data.frame(Variables = row.names(importance),
Importance = round(importance[ ,'MeanDecreaseGini'],2))
rankImportance <- varImportance %>%
mutate(Rank = paste0('#',dense_rank(desc(Importance))))
# Visualizziamo il tutto con ggplot
ggplot(rankImportance, aes(x = reorder(Variables, Importance),
y = Importance, fill = Importance)) +
geom_bar(stat='identity') +
geom_text(aes(x = Variables, y = 0.5, label = Rank),
hjust=0, vjust=0.55, size = 4, colour = 'red') +
labs(x = 'Regressors') +
coord_flip() +
theme_classic()
predictions <- predict(rf_model, newdata = test_set)
cm2<-confusionMatrix(predictions, as.factor(y_test))
cm2
plot(rf_model, ylim=c(0,0.50), xlim=c(0,500))
legend('topright', colnames(rf_model$err.rate), col=1:3, fill=1:3)
plot(rf_model, ylim=c(0,0.50), xlim=c(0,140))
legend('topright', colnames(rf_model$err.rate), col=1:3, fill=1:3)
rf_model = randomForest(as.factor(y_training)~. , data = training_set, ntree =450, importance = TRUE)
# Classifica di importanza dei vari regressori ai fini della predizione
importance <- importance(rf_model)
varImportance <- data.frame(Variables = row.names(importance),
Importance = round(importance[ ,'MeanDecreaseGini'],2))
rankImportance <- varImportance %>%
mutate(Rank = paste0('#',dense_rank(desc(Importance))))
# Visualizziamo il tutto con ggplot
ggplot(rankImportance, aes(x = reorder(Variables, Importance),
y = Importance, fill = Importance)) +
geom_bar(stat='identity') +
geom_text(aes(x = Variables, y = 0.5, label = Rank),
hjust=0, vjust=0.55, size = 4, colour = 'red') +
labs(x = 'Regressors') +
coord_flip() +
theme_classic()
predictions <- predict(rf_model, newdata = test_set)
cm2<-confusionMatrix(predictions, as.factor(y_test))
cm2
plot(rf_model, ylim=c(0,0.50), xlim=c(0,500))
legend('topright', colnames(rf_model$err.rate), col=1:3, fill=1:3)
plot(rf_model, ylim=c(0,0.50), xlim=c(0,140))
legend('topright', colnames(rf_model$err.rate), col=1:3, fill=1:3)
set.seed(100)
x <- DataSet
#Standardizzazione
#la standardizzazione non è necessaria per i regressori con valori caratterizzati da una scala limitata
#regressori da standardizzare
x_toscale <- x[, c("age", "trtbps", "chol", "thalachh", "oldpeak")]
#regressori da non standardizzare
dropped <- x[,  c("sex", "cp", "fbs", "restecg", "exng", "slp", "caa", "thall")]
head(dropped)
head(x_toscale)
scaled_regressors <- scale(x_toscale) #standardizziamo
head(scaled_regressors)
cleaned_regressors <- cbind(scaled_regressors, dropped, label = x$output) #dataset completo standardizzato
head(cleaned_regressors)
#divisione in training set e test set
index <- sample(1:nrow(cleaned_regressors), 0.7*nrow(cleaned_regressors))
training_set <- cleaned_regressors[index, ]
y_training <- training_set[, "label"] #var dipendente training
training_set <- training_set[,-14]
test_set <- cleaned_regressors[-index, ]
y_test <- test_set[, "label"] #var dipendente test
test_set <- test_set[, -14] #eliminiamo la variabile dipendente dal test set
head(test_set)
dim(training_set)
dim(test_set)
summary(training_set)
summary(test_set)
############# RANDOM FOREST CLASSIFIER ################
rf_model = randomForest(as.factor(y_training)~. , data = training_set, ntree =450, importance = TRUE)
# Classifica di importanza dei vari regressori ai fini della predizione
importance <- importance(rf_model)
varImportance <- data.frame(Variables = row.names(importance),
Importance = round(importance[ ,'MeanDecreaseGini'],2))
rankImportance <- varImportance %>%
mutate(Rank = paste0('#',dense_rank(desc(Importance))))
# Visualizziamo il tutto con ggplot
ggplot(rankImportance, aes(x = reorder(Variables, Importance),
y = Importance, fill = Importance)) +
geom_bar(stat='identity') +
geom_text(aes(x = Variables, y = 0.5, label = Rank),
hjust=0, vjust=0.55, size = 4, colour = 'red') +
labs(x = 'Regressors') +
coord_flip() +
theme_classic()
predictions <- predict(rf_model, newdata = test_set)
cm2<-confusionMatrix(predictions, as.factor(y_test))
cm2
plot(rf_model, ylim=c(0,0.50), xlim=c(0,500))
legend('topright', colnames(rf_model$err.rate), col=1:3, fill=1:3)
plot(rf_model, ylim=c(0,0.50), xlim=c(0,140))
legend('topright', colnames(rf_model$err.rate), col=1:3, fill=1:3)
plot(rf_model, ylim=c(0,0.50), xlim=c(0,450))
legend('topright', colnames(rf_model$err.rate), col=1:3, fill=1:3)
plot(rf_model, ylim=c(0,0.50), xlim=c(0,140))
legend('topright', colnames(rf_model$err.rate), col=1:3, fill=1:3)
